# -*- coding: utf-8 -*-
"""Mod_11_Web_Scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxA4N0Zc-mDtZmQKYPiYXil1aIfBy_8R
"""

import requests #library used to access the content from websites
from bs4 import BeautifulSoup #library used for parsing the code

#scrape and print all the references from nau.edu
URL = 'https://nau.edu/' #the url contains the website we want to scrape
response = requests.get(URL)
soup = BeautifulSoup(response.text, 'lxml')
#print(response.text)
all_urls = soup.find_all('a')
#print(all_urls)
for each in all_urls:
  print(each['href'])
  #.text removes the tags and only prints the text of the code

import requests #library used to access the content from websites
from bs4 import BeautifulSoup #library used for parsing the code

#scrape and print dataset information from data.gov
URL = 'https://catalog.data.gov/dataset'
response = requests.get(URL)
soup = BeautifulSoup(response.text, 'lxml')


#create lists to hold all of the items for each element
titles = []
descriptions = []
sources = []

#scrape the titles for each dataset
all_urls = soup.find_all('h3', class_='dataset-heading') #note the underscore after class
for each in all_urls:
 #print(each.a.text)
 titles.append(each.a.text)

#scrape the descriptions for each dataset
all_urls = soup.find_all('div', class_='notes') #note the underscore after class
for each in all_urls:
  #print(each.div.text)
  descriptions.append(each.div.text)

#scrape the source for each dataset
all_urls = soup.find_all('div', class_='notes') #note the underscore after class
for each in all_urls:
  #print(each.p.text[:-2])
  sources.append(each.p.text[:-2])

#print all of the elements of the list together
i = 0
for t in titles:
  print(f'Title: {t}')
  print(f'Description: {descriptions[i]}')
  print(f'Source: {sources[i]}\n')
  i += 1