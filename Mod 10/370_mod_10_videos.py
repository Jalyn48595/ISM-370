# -*- coding: utf-8 -*-
"""370_Mod_10_Videos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Ap5UlDj6CiRSnAEshL2BNJR24nxJFgT

Description: Using supervised learning and a variety of models to determine the success of an online article.
Created by: Jalyn Buthman
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#modify pandas display options to show all columns and not wrap
pd.set_option('display.max_columns', None)
pd.set_option('display.expand_frame_repr', False)

#read in the data
news = pd.read_csv("/content/OnlineNewsPopularity.csv")
print(news.head())
print()

#target variable for this dataset: shares
#create a box plot showing the relationship between sentiment and shares
#column names inlcude a leading space in the dataset
#global sentiment polarity represents how positive/negative an article is
plt.scatter(news[' global_sentiment_polarity'], news[' shares'])
plt.title('Popularity by Sentiment')
plt.xlabel('Sentiment Polarity')
plt.ylabel('Shares')
plt.show()
print()

#use linear regression to test for a relationship
from sklearn.linear_model import LinearRegression
x = news[' global_sentiment_polarity'].values.reshape(-1, 1)
y = news[' shares'].values.reshape(-1, 1)
regressor = LinearRegression()
regressor.fit(x, y)

#plot the regression line
regline = regressor.predict(x)
plt.scatter(news[' global_sentiment_polarity'], news[' shares'], color = 'blue')
plt.plot(sorted(news[' global_sentiment_polarity'].tolist()), regline, 'r')
plt.title('Shares by Sentiment')
plt.xlabel('Sentiment')
plt.ylabel('Shares')
plt.show()
print()
#you cannot use a flat regression line to predict the outcome variable, move onto another model



#k Nearest Neighbor, try to identify articles that have a similar sentiment and see how many shares it has
#start by defining k (the number of neighbors) and target value
k = 15
newsentiment = 0.5

#convert all sentiments and shares to lists
allsentiment = news[' global_sentiment_polarity'].tolist()
allshares = news[' shares'].tolist()

#create the x and y variable
x= np.array(allsentiment).reshape(-1, 1)
y = np.array(allshares)

#create a regressor and fit it to our data
from sklearn.neighbors import KNeighborsRegressor
knnregressor = KNeighborsRegressor(n_neighbors = k)
knnregressor.fit (x, y)

#print the prediction for shares for the sentiment score
print('Mean shares of the 15 nearest neighbors: ')
print(knnregressor.predict(np.array(newsentiment).reshape(1, -1)))
print()
#the 15 articles that had a have a sentiment closest to 0.5 had a sentiment score of 7,344
#compare this model to other models to see how accurate this prediction is


#decision tree
#max depth is how complicated you want the analysis to be
from sklearn.tree import DecisionTreeRegressor
dtregressor = DecisionTreeRegressor (max_depth = 3)
dtregressor.fit (np.array(allsentiment).reshape(-1, 1), np.array(allshares))
print('My decision tree prediction of number of shares: ')
print(dtregressor.predict(np.array(newsentiment).reshape(1, -1)))
print()
#the decision tree outcome is much lower than the k nearest neighbor model


#random forest
#may take a lot of computing power
from sklearn.ensemble import RandomForestRegressor
rfregressor = RandomForestRegressor()
rfregressor.fit(np.array(allsentiment).reshape(-1, 1), np.array(allshares))
print('Random forest prediction of number of shares: ')
print(rfregressor.predict(np.array(newsentiment).reshape(1, -1)))
print()
#random forest's prediction is in between the decision tree and k nearest neighbor values


#neural network
from sklearn.neural_network import MLPRegressor
nnregressor = MLPRegressor()
nnregressor.fit(np.array(allsentiment).reshape(-1, 1), np.array(allshares))
print('Neural network prediction of number of shares: ')
print(nnregressor.predict(np.array(newsentiment).reshape(1, -1)))
print()


#accuracy testing
#create a train, test data with 75/25 split, used to train data and test accuracy
from sklearn.model_selection import train_test_split
x = np.array(allsentiment).reshape(-1, 1)
y= np.array(allshares)
trainingx, testx, trainingy, testy = train_test_split(x, y, random_state = 1)


#use each technique on the train/test data to see which is the most accurate/smallest error

#linear regression
regressor = LinearRegression()
regressor.fit (trainingx, trainingy)
predicted = regressor.predict(testx)
predictionerror = abs(predicted - testy)
print('Linear regression prediction score: ')
print(np.mean(predictionerror))
print()

#K nearest neighbor
knnregressor = KNeighborsRegressor(n_neighbors = 15)
knnregressor.fit (trainingx, trainingy)
predicted = knnregressor.predict(testx)
predictionerror = abs(predicted - testy)
print('Nearest neighbor prediction score: ')
print(np.mean(predictionerror))
print()

#decision trees
dtregressor = DecisionTreeRegressor(max_depth = 3)
dtregressor.fit (trainingx, trainingy)
predicted = dtregressor.predict(testx)
predictionerror = abs(predicted - testy)
print('Decision Tree prediction score: ')
print(np.mean(predictionerror))
print()

#random forest
rfregressor = RandomForestRegressor(random_state = 1)
rfregressor.fit (trainingx, trainingy)
predicted = rfregressor.predict(testx)
predictionerror = abs(predicted - testy)
print('Random forest prediction score: ')
print(np.mean(predictionerror))
print()

#neural network
nnregressor = MLPRegressor()
nnregressor.fit (trainingx, trainingy)
predicted = rfregressor.predict(testx)
predictionerror = abs(predicted - testy)
print('Neural network prediction score: ')
print(np.mean(predictionerror))
print()

#check which supervised learning technique has the least amount of errors to choose which model to use for predictions