# -*- coding: utf-8 -*-
"""370_Mod_7_Videos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S9zbilKNVnxHB1Q9XLJUEQQULpdocP1l

Description: This project demonstrates how to conduct binary classification with logistic regression in Python.
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

#read in the data set
attrition_past = pd.read_csv("/content/attrition_past.csv")

print(attrition_past.head())
print()
print(attrition_past.shape)
print()
print(attrition_past.describe())
print()

#lastmonth_activity is how many times the corp accessed the software in the last month
#lastyear_activity is how many times the corp accessed the software in the last year
#exited is a binary variable: 1 = the cx exited/cancelled the subsription, 0= the cx did not cancel

#for both last year stats the mean is much higher than the median, so go with the median
#since excited mean is higher than .5 more than half of cx have exited

#use logistic regression to predict attrition/churn
#linear regression predicts continuous variables (could be any outcome)
#logistic is better at predicting binary outcomes
model = LogisticRegression(solver= 'liblinear', random_state = 0)
x = attrition_past ['lastmonth_activity'].values.reshape(-1, 1)
y = attrition_past ['exited'].values.reshape(-1, 1)
model.fit(x, y)

#generate predicted probabilities by adding a new column
attrition_past['logistic_prediction']= model.predict_proba(x)[:,1]

#print the header to look at logistic regression probabilities
print(attrition_past.head())
print()

#look at the top 10 companies most likely to exit
print('The 10 highest probabilities of exiting:')
print(attrition_past.nlargest(10, 'logistic_prediction'))
print()

#create a confusion matrix, 2x2 matrix showing what percentage of predictions came true
the_median = attrition_past['logistic_prediction'].median()
#print(the_median)
#create a list where anything above the median is 1, anything below is 0
prediction = list(1 * attrition_past['logistic_prediction'] > the_median)
#create a list to see if the companies actually exited or not
actual = list(attrition_past['exited'])
#create the matrix, true positive: predicted to exit, did exit. false positive: predicted to exit, did not
#false negatives: predicted to stay, exited. true negatives: predicted to stay, did stay
print('[true positive false positives]')
print ('[false negatives true negatives]')
print(confusion_matrix(prediction, actual))
print()

#calculate accuracy scores of predictions from the confusion matrix
#precision - how many were actually positive out of what we though would be positive?
#recall (sensativity) - from the true positive cases, how many did we think were positive?
#you want precision and recall to be close to 1

#store the confision matrix in an object
conf_mat = confusion_matrix(prediction, actual)

#conf_mat [row][column]
#true positive / (true positive + false positive)
precision = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[0][1])
print(f'Precision: {precision}')
print()
#true positive / (true positive + false negatives)
recall = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[1][0])
print(f'Recall: {recall}')
print()

#if you are not very accurate consider adding additional variables to the model, see below for edited code

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix

#read in the data set
attrition_past = pd.read_csv("/content/attrition_past.csv")

print(attrition_past.head())
print()
print(attrition_past.shape)
print()
print(attrition_past.describe())
print()

#lastmonth_activity is how many times the corp accessed the software in the last month
#lastyear_activity is how many times the corp accessed the software in the last year
#exited is a binary variable: 1 = the cx exited/cancelled the subsription, 0= the cx did not cancel

#for both last year stats the mean is much higher than the median, so go with the median
#since excited mean is higher than .5 more than half of cx have exited

#use logistic regression to predict attrition/churn
#linear regression predicts continuous variables (could be any outcome)
#logistic is better at predicting binary outcomes

#when using two or more variables you run the code below without .reshape() after .values
#add a second set of square brackets and the new variable under x =

model = LogisticRegression(solver= 'liblinear', random_state = 0)
x = attrition_past [['lastmonth_activity', 'number_of_employees']].values
y = attrition_past ['exited'].values
model.fit(x, y)

#generate predicted probabilities by adding a new column
attrition_past['logistic_prediction']= model.predict_proba(x)[:,1]

#print the header to look at logistic regression probabilities
print(attrition_past.head())
print()

#look at the top 10 companies most likely to exit
print('The 10 highest probabilities of exiting:')
print(attrition_past.nlargest(10, 'logistic_prediction'))
print()

#create a confusion matrix, 2x2 matrix showing what percentage of predictions came true
the_median = attrition_past['logistic_prediction'].median()
#print(the_median)
#create a list where anything above the median is 1, anything below is 0
prediction = list(1 * attrition_past['logistic_prediction'] > the_median)
#create a list to see if the companies actually exited or not
actual = list(attrition_past['exited'])
#create the matrix, true positive: predicted to exit, did exit. false positive: predicted to exit, did not
#false negatives: predicted to stay, exited. true negatives: predicted to stay, did stay
print('[true positive false positives]')
print ('[false negatives true negatives]')
print(confusion_matrix(prediction, actual))
print()

#calculate accuracy scores of predictions from the confusion matrix
#precision - how many were actually positive out of what we though would be positive?
#recall (sensativity) - from the true positive cases, how many did we think were positive?
#you want precision and recall to be close to 1

#store the confision matrix in an object
conf_mat = confusion_matrix(prediction, actual)

#conf_mat [row][column]
#true positive / (true positive + false positive)
precision = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[0][1])
print(f'Precision: {precision}')
print()
#true positive / (true positive + false negatives)
recall = conf_mat[0][0] / (conf_mat[0][0] + conf_mat[1][0])
print(f'Recall: {recall}')
print()


#Apply the above model and probabilities to a new dataset
attrition_future = pd.read_csv ("/content/attrition2.csv")

print(attrition_future.head())
print(attrition_future.shape)
print(attrition_future.describe())
print()

#duplicate the model above with the new dataset
x2 = attrition_future[['lastmonth_activity', 'number_of_employees']].values
attrition_future ['logistic_prediction'] = model.predict_proba(x2)[:,1]
print(attrition_future.head())
print()
#print the top five highest predicted values
print('Top five companies most likely to exit:')
print(attrition_future.nlargest(5, 'logistic_prediction'))
print()